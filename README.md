# ğŸ§  Agent Evaluation metrics

This project explores how to evaluate the performance of LLM-based agents using **context inclusion and factual check scoring**â€”a method to check whether an agent's output meaningfully reflects the expected context.

Traditional ML models use metrics like Precision, Recall, and F1-score. But for agents that generate text, we need smarter ways to measure quality. This weekend project was born out of curiosity to answer:  
**"How do I know if my agent is actually using the context it retrieved and the agent response is factually accurate?"**

---

## ğŸš€ What This Project Does

- Extracts **contextual keywords** from a reference sentence using [KeyBERT](https://github.com/MaartenGr/KeyBERT)
- Compares those keywords against the agent's output
- Computes a **context inclusion score**: the fraction of expected keywords found in the output
- Derives BERT Factuality Score
- Prints detailed evaluation results for each sample

---

## ğŸ“¦ Dependencies

Install these in your Python environment or Colab notebook:

```bash
pip install keybert
pip install sentence-transformers

Sample Evaluation
from keybert import KeyBERT
kw_model = KeyBERT()

expected = "Grilled chicken with olive oil and lemon juice is a healthy dinner option."
output = "You can make grilled chicken by marinating it in lemon juice and olive oil before grilling."

keywords = kw_model.extract_keywords(expected, top_n=5, stop_words='english')
filtered_keywords = [kw for kw, _ in keywords]
score = len([kw for kw in filtered_keywords if kw.lower() in output.lower()]) / len(filtered_keywords)
print("Context Inclusion Score:", round(score, 2))

---

ğŸ“ Files
- agent_evaluation.py: Main script to run evaluation metrics
- README.md: Project overview and instructions

ğŸ’¡ Future Extensions
- Sometimes, an LLM might parrot key words without actually understanding the userâ€™s input.
- Evaluate prompt chain when one LLM talks to another LLM.
- Routing accuracy when system classifies user inputs into distinct routes (LLMs).
- Evaluate agents working in Parallel, running multiple tools or subtasks simultaneously.
- Evaluate agents where one agent generates content and other agent acts as a critic to evaluates the content generated by other agent.

ğŸ™‹â€â™‚ï¸ Author
Shravan Mateti
Curious about practical AI, agent reliability, and educational analytics.
Connect with me on LinkedIn to share ideas or feedback!

ğŸ“œ License
This project is open-source under the MIT License. Feel free to fork, extend, or contribute!
