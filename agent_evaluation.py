# -*- coding: utf-8 -*-
"""Agent Evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aLCLusrRY-u4lczmUGcJISpwZiABJUgI

# Context Inclusion

In agentic systems, especially those using retrieval-augmented generation (RAG), it's crucial to verify that the retrieved context actually influences the final output. Otherwise, the augmentation step is wasted. This function works as below:


*   Extracts top keywords from the expected sentence using KeyBERT.
*   Derives relevance score and ranks.
*   Checks if each keyword is present in the output.
*   Returns a score: #keywords found / #keywords extracted.
"""

# Step 1: Install KeyBERT and dependencies
!pip install -q keybert
!pip install -q sentence-transformers

# Step 2: Import modules
from keybert import KeyBERT
kw_model = KeyBERT()

# Step 3: Define context inclusion scorer using keyword presence
def context_inclusion_keywords(expected: str, output: str, top_n: int = 5, min_score: float = 0.5) -> float:
    """
    Extracts keywords from expected using KeyBERT and checks if they appear in output.

    Args:
        expected: Ground truth sentence
        output: Agent's response
        top_n: Number of top keywords to extract
        min_score: Minimum relevance score to consider a keyword

    Returns:
        Inclusion score: fraction of keywords found in output
    """
    keywords = kw_model.extract_keywords(expected, top_n=top_n, stop_words='english')
    filtered_keywords = [kw for kw, score in keywords if score >= min_score]
    found = [kw for kw in filtered_keywords if kw.lower() in output.lower()]
    return round(len(found) / len(filtered_keywords), 2) if filtered_keywords else None

# ðŸ§ª Step 4: Sample dataset
dataset = [
    {
        "input": "Can you give me a recipe for grilled chicken?",
        "expected": "Grilled chicken with olive oil and lemon juice is a healthy dinner option.",
        "output": "You can make grilled chicken by marinating it in lemon juice and olive oil before grilling."
    },
    {
        "input": "What ingredients are used in grilled chicken?",
        "expected": "The recipe includes garlic, olive oil, lemon juice, and chicken breasts.",
        "output": "Use chicken breasts, garlic, and lemon juice for flavor."
    },
    {
        "input": "Is grilled chicken healthy?",
        "expected": "Grilled chicken is a lean protein and low in fat, making it a healthy choice.",
        "output": "Yes, grilled chicken is a good source of lean protein and low in fat."
    }
]

# ðŸ“ˆ Step 5: Run context inclusion evaluation with detailed output
for i, item in enumerate(dataset, start=1):
    # Extract keywords from expected
    raw_keywords = kw_model.extract_keywords(item["expected"], top_n=5, stop_words='english')
    filtered_keywords = [kw for kw, _ in raw_keywords]  # No score filtering

    # Compute inclusion score
    found = [kw for kw in filtered_keywords if kw.lower() in item["output"].lower()]
    score = round(len(found) / len(filtered_keywords), 2) if filtered_keywords else None

    # Display results
    print(f"ðŸ”¹ Example {i}")
    print(f"Input: {item['input']}")
    print(f"Expected: {item['expected']}")
    print(f"Expected Keywords: {filtered_keywords}")
    print(f"Output: {item['output']}")
    print(f"Context Inclusion Score: {score}\n")

"""# Factuality

Factuality measures whether the agentâ€™s output is accurate and consistent with the ground truth or source material. The scorer compares the agentâ€™s output against the expected field and flags discrepancies. Itâ€™s crucial for:

*   Retrieval-augmented agents: Ensuring that retrieved content is faithfully reflected in the output.
*   Instruction-following agents: Verifying that generated responses donâ€™t hallucinate or misrepresent facts.
*   Safety and reliability: Preventing misinformation, especially in domains like health, finance, or education.

Let's derive factuality scorer using BERT-based semantic similarity. BERT understands context, word meaning, and sentence structureâ€”so it can detect factual alignment even when the wording is different.

Bert Factuality Score:

*   More than 0.95 - Very close match
*   Between 0.70 to 0.95 - Partial factual overlap
*   Less than 0.7 - Likely missing or incorrect facts
"""

# Step 1: Install required libraries
!pip install -q sentence-transformers

# Step 2: Import modules
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# Step 3: Load BERT model (MiniLM is fast and accurate)
model = SentenceTransformer('all-MiniLM-L6-v2')

# Step 4: Define BERT-based factuality scorer
def factuality_score_bert(output: str, expected: str) -> float:
    """
    Computes cosine similarity between BERT embeddings of output and expected text.

    Args:
        output: Agent's response
        expected: Ground truth reference

    Returns:
        Cosine similarity score between 0.0 and 1.0
    """
    embeddings = model.encode([output, expected])
    similarity = cosine_similarity([embeddings[0]], [embeddings[1]])
    return round(float(similarity[0][0]), 4)

# Step 5: Sample dataset
dataset = [
    {
        "input": "Can you give me a recipe for grilled chicken?",
        "expected": """Marinate chicken with olive oil, garlic, lemon juice, and spices. Grill over medium-high heat for 6-8 minutes per side.""",
        "output": """Grill chicken after marinating in olive oil, garlic, lemon juice, and spices for 30 minutes."""
    },
    {
        "input": "How long should I marinate chicken for grilling?",
        "expected": """Marinate chicken for 30 minutes to 2 hours. Avoid over-marinating beyond 24 hours.""",
        "output": """Marinate chicken for 30 minutes to 2 hours. Over-marinating can affect texture."""
    },
    {
        "input": "What sides go well with grilled chicken?",
        "expected": """Grilled chicken pairs well with roasted vegetables, mashed potatoes, or quinoa salad.""",
        "output": """Try mashed potatoes, grilled corn, or a fresh salad with grilled chicken."""
    }
]

# Step 6: Run evaluation
for i, item in enumerate(dataset, start=1):
    score = factuality_score_bert(item["output"], item["expected"])
    print(f"Example {i}:")
    print(f"Input: {item['input']}")
    print(f"Output: {item['output']}")
    print(f"Expected: {item['expected']}")
    print(f"BERT Factuality Score: {score}\n")